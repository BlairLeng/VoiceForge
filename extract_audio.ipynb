{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install cn_clip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hprCr58VYErb",
        "outputId": "76a81da4-0396-44f2-e468-ea8e6102bd9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cn_clip in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cn_clip) (1.23.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from cn_clip) (4.66.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from cn_clip) (1.16.0)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from cn_clip) (0.9.6)\n",
            "Requirement already satisfied: lmdb==1.3.0 in /usr/local/lib/python3.10/dist-packages (from cn_clip) (1.3.0)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from cn_clip) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from cn_clip) (0.15.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->cn_clip) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->cn_clip) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->cn_clip) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->cn_clip) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->cn_clip) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.1->cn_clip) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.1->cn_clip) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.1->cn_clip) (16.0.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->cn_clip) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm->cn_clip) (0.16.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->cn_clip) (0.3.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->cn_clip) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->cn_clip) (9.4.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm->cn_clip) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm->cn_clip) (23.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.1->cn_clip) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->cn_clip) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->cn_clip) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->cn_clip) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->cn_clip) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.1->cn_clip) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2ym87Q4PRrK",
        "outputId": "42f2dfc5-42ab-45b4-e7ff-106ca5413d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4DsdgI57ezY"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "def get_avg_time(time_1, time_2):\n",
        "    # 将时间字符串转换为时间间隔\n",
        "    format_string = '%H:%M:%S.%f'\n",
        "    time1_obj = datetime.strptime(time_1, format_string)\n",
        "    time2_obj = datetime.strptime(time_2, format_string)\n",
        "\n",
        "    # 计算时间间隔的总和\n",
        "    total_time = time1_obj - datetime.min + (time2_obj - datetime.min)\n",
        "\n",
        "    # 计算平均时间间隔\n",
        "    average_time = total_time / 2\n",
        "\n",
        "    # 将平均时间间隔格式化为相同的时间字符串格式\n",
        "    average_time_str = (datetime.min + average_time).time().strftime(format_string)\n",
        "    return average_time_str"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getTimeAndText(ass_path):\n",
        "    with open(ass_path, 'r', encoding='utf-16') as file:\n",
        "        lines = file.readlines()\n",
        "    keyword = \"zhengwen\"\n",
        "    avg_time_list = []\n",
        "    text_list = []\n",
        "    for line in lines:\n",
        "        if line.startswith('Dialogue') and keyword in line:\n",
        "            content = line.split(\":\", 1)[1].strip()\n",
        "            line_list = content.split(\",\")\n",
        "            avg_time = get_avg_time(line_list[1], line_list[2])\n",
        "            avg_time_list.append(avg_time)\n",
        "            text_list.append(line_list[-1])\n",
        "    return avg_time_list, text_list"
      ],
      "metadata": {
        "id": "6sZv5EqZ727K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "import cn_clip.clip as clip\n",
        "from cn_clip.clip import tokenize, load_from_name"
      ],
      "metadata": {
        "id": "5ZoU3yIjYJzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, processer = load_from_name(\"ViT-H-14\", device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7ynscaMYO1R",
        "outputId": "ce049be2-0a55-4837-eb4e-98d3b891740d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 3.57G/3.57G [01:42<00:00, 37.5MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading vision model config from /usr/local/lib/python3.10/dist-packages/cn_clip/clip/model_configs/ViT-H-14.json\n",
            "Loading text model config from /usr/local/lib/python3.10/dist-packages/cn_clip/clip/model_configs/RoBERTa-wwm-ext-large-chinese.json\n",
            "Model info {'embed_dim': 1024, 'image_resolution': 224, 'vision_layers': 32, 'vision_width': 1280, 'vision_head_width': 80, 'vision_patch_size': 14, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 1024, 'text_initializer_range': 0.02, 'text_intermediate_size': 4096, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 16, 'text_num_hidden_layers': 24, 'text_type_vocab_size': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhUXECVHYcJ4",
        "outputId": "a1839168-8597-4999-fe93-da9d9f76d79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIP(\n",
              "  (visual): VisualTransformer(\n",
              "    (conv1): Conv2d(3, 1280, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
              "    (ln_pre): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (12): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (13): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (14): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (15): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (16): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (17): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (18): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (19): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (20): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (21): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (22): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (23): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (24): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (25): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (26): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (27): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (28): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (29): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (30): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (31): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(21128, 1024, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 1024)\n",
              "      (token_type_embeddings): Embedding(2, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import subprocess\n",
        "\n",
        "def extract_frame_at_time(video_path, output_path, time_stamp):\n",
        "    ffmpeg_cmd = [\n",
        "        'ffmpeg',\n",
        "        '-ss', time_stamp,\n",
        "        '-i', video_path,\n",
        "        '-vframes', '1',\n",
        "        f\"\\'{output_path}\\'\"\n",
        "    ]\n",
        "\n",
        "    subprocess.run(ffmpeg_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "# for id in [str(x).zfill(2) for x in range(15, 29)]:\n",
        "\n",
        "for k in range(1,15):\n",
        "    save_path = f\"/content/drive/MyDrive/GPTData/Haruhi_data/{k:0=2d}/\"\n",
        "    if not os.path.exists(save_path):\n",
        "        os.mkdir(save_path)\n",
        "    data = []\n",
        "    ass_path = f\"[CASO][Suzumiya_Haruhi_no_Yuuutsu][{k:0=2d}]\"\n",
        "    video_path = f\"[CASO][Suzumiya_Haruhi_no_Yuuutsu][{k:0=2d}]\"\n",
        "\n",
        "    directory = \"/content/drive/MyDrive/GPTData/Haruhi_all\"\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\"ass\") and filename.startswith(ass_path):\n",
        "            ass_path = filename\n",
        "        if filename.endswith(\"mkv\") and filename.startswith(video_path):\n",
        "            video_path = filename\n",
        "\n",
        "    ass_path = os.path.join(directory, ass_path)\n",
        "    video_path = os.path.join(directory, video_path)\n",
        "\n",
        "    avg_time_list, text_list = getTimeAndText(ass_path)\n",
        "\n",
        "    for i in range(len(avg_time_list)):\n",
        "        time_stamp = avg_time_list[i]\n",
        "        output_path = save_path + f'{text_list[i]}{avg_time_list[i]}.jpg'\n",
        "        extract_frame_at_time(video_path, output_path, time_stamp)\n",
        "        data.append({\"text\":str(text_list[i]), \"timestamp\":str(avg_time_list[i]), \"path\":f\"{text_list[i]}{avg_time_list[i]}.jpg\"})\n",
        "\n",
        "    with open(f'{save_path}/info_{k:0=2d}.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "xeCLyistPQRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(1,15):\n",
        "    save_path = f\"/content/drive/MyDrive/GPTData/Haruhi_data/{k:0=2d}/\"\n",
        "    base_dir = save_path\n",
        "\n",
        "    info_dir = os.path.join(save_path, f\"info_{k:0=2d}.json\")\n",
        "\n",
        "    data = []\n",
        "\n",
        "    with open(info_dir, 'r', encoding='utf-8') as f:\n",
        "        info = json.load(f)\n",
        "\n",
        "    for item in info:\n",
        "        image = processer(Image.open(f\"{base_dir}/{item['path']}\")).unsqueeze(0).to(device)\n",
        "        text  = tokenize([item['text']]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_embedding = model.encode_image(image)\n",
        "            text_embedding  = model.encode_text(text)\n",
        "\n",
        "            image_embedding /= image_embedding.norm(dim=-1, keepdim=True)\n",
        "            text_embedding  /= text_embedding.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        item['text_embedding'] = text_embedding[0].tolist()\n",
        "        item['image_embedding'] = image_embedding[0].tolist()\n",
        "\n",
        "        data.append(item)\n",
        "\n",
        "    with open(os.path.join(save_path, f\"info_{k:0=2d}_embedding.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(data,f,ensure_ascii=False,indent=4)"
      ],
      "metadata": {
        "id": "i7KD1ldm_uvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AMKsH1DW-Oqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vzbyB_RgHg3c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}