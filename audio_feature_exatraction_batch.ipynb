{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import requests\n",
    "import fnmatch\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/BlairLeng/VoiceForge.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "\n",
    "import requests\n",
    "from VoiceForge.audio_feature_ext.modules.ecapa_tdnn import EcapaTdnn, SpeakerIdetification\n",
    "from VoiceForge.audio_feature_ext.data_utils.reader import load_audio, CustomDataset\n",
    "\n",
    "class AudioFeatureExtraction:\n",
    "    def __init__(self,model_director='./audio_feature_ext/models', feature_method='melspectrogram' ):\n",
    "        self.use_model = ''\n",
    "        self.model_director = model_director\n",
    "        self.feature_method = feature_method\n",
    "        self.model = None\n",
    "        self.device = None\n",
    "        self.load_model()\n",
    "\n",
    "    def init_models(self,path):\n",
    "        model_urls = ['https://huggingface.co/scixing/voicemodel/resolve/main/model.pth',\n",
    "                      'https://huggingface.co/scixing/voicemodel/resolve/main/model.state',\n",
    "                      'https://huggingface.co/scixing/voicemodel/resolve/main/optimizer.pth']\n",
    "        listdir = os.listdir(path)\n",
    "        for url in model_urls:\n",
    "            filename = url.split('/')[-1]\n",
    "            if filename in listdir:\n",
    "                continue\n",
    "            r = requests.get(url, allow_redirects=True)\n",
    "            print(f'downloading model pth {filename}')\n",
    "            open(f'{path}/{filename}', 'wb').write(r.content)\n",
    "            print(f'{filename} success download')\n",
    "\n",
    "    def load_model(self):\n",
    "        dataset = CustomDataset(data_list_path=None, feature_method=self.feature_method)\n",
    "        ecapa_tdnn = EcapaTdnn(input_size=dataset.input_size)\n",
    "        self.model = SpeakerIdetification(backbone=ecapa_tdnn)\n",
    "        self.device = torch.device(\"cuda\")\n",
    "\n",
    "        if not os.path.exists(self.model_director):\n",
    "            os.makedirs(self.model_director)\n",
    "        model_files = ['model.pth', 'model.state', 'optimizer.pth']\n",
    "        for file in model_files:\n",
    "            if not os.path.exists(f'{self.model_director}/{file}'):\n",
    "                self.init_models(self.model_director)\n",
    "\n",
    "        # 加载模型\n",
    "        model_path = os.path.join(self.model_director, 'model.pth')\n",
    "        model_dict = self.model.state_dict()\n",
    "        param_state_dict = torch.load(model_path)\n",
    "        for name, weight in model_dict.items():\n",
    "            if name in param_state_dict.keys():\n",
    "                if list(weight.shape) != list(param_state_dict[name].shape):\n",
    "                    param_state_dict.pop(name, None)\n",
    "        self.model.load_state_dict(param_state_dict, strict=False)\n",
    "        print(f\"成功加载模型参数和优化方法参数：{model_path}\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def infer(self, audio_path, duration):\n",
    "        data = load_audio(audio_path, mode='infer', feature_method=self.feature_method,\n",
    "                          chunk_duration=duration)\n",
    "        data = data[np.newaxis, :]\n",
    "        data = torch.tensor(data, dtype=torch.float32, device=self.device)\n",
    "        feature = self.model.backbone(data)\n",
    "        return feature.data.detach().cpu().numpy()\n",
    "\n",
    "    def feats_extract(self, dataloader):\n",
    "        feat_list = []\n",
    "        for data in dataloader:\n",
    "            data = data.to(self.device)\n",
    "            feats = self.model.backbone(data)\n",
    "            feat_list.append(feats.data.detach().cpu().numpy())\n",
    "        return np.concatenate(np.array(feat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDefDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    加载并预处理音频\n",
    "    :param data_list_path: 数据列表\n",
    "    :param feature_method: 预处理方法\n",
    "    :param mode: 对数据处理的方式，包括train，eval，infer\n",
    "    :param sr: 采样率\n",
    "    :param chunk_duration: 训练或者评估使用的音频长度\n",
    "    :param min_duration: 最小训练或者评估的音频长度\n",
    "    :param augmentors: 数据增强方法\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list,\n",
    "                 feature_method='melspectrogram',\n",
    "                 mode='eval',\n",
    "                 sr=16000,\n",
    "                 chunk_duration=2,\n",
    "                 min_duration=2,\n",
    "                 augmentors=None):\n",
    "        super(CustomDefDataset, self).__init__()\n",
    "        self.lines = data_list\n",
    "        # self.lines = data_list_path\n",
    "        self.feature_method = feature_method\n",
    "        self.mode = mode\n",
    "        self.sr = sr\n",
    "        self.chunk_duration = chunk_duration\n",
    "        self.min_duration = min_duration\n",
    "        self.augmentors = augmentors\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            audio_path = self.lines[idx]\n",
    "            # print(audio_path)\n",
    "            # 加载并预处理音频\n",
    "            features = load_audio(audio_path, feature_method=self.feature_method, mode=self.mode, sr=self.sr,\n",
    "                                  chunk_duration=self.chunk_duration, min_duration=self.min_duration,\n",
    "                                  augmentors=self.augmentors)\n",
    "            # print(features.shape)\n",
    "            return features\n",
    "        except Exception as ex:\n",
    "            print(f\"[{datetime.now()}] 数据: {self.lines[idx]} 出错，错误信息: {ex}\", file=sys.stderr)\n",
    "            rnd_idx = np.random.randint(self.__len__())\n",
    "            return self.__getitem__(rnd_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    @property\n",
    "    def input_size(self):\n",
    "        if self.feature_method == 'melspectrogram':\n",
    "            return 80\n",
    "        elif self.feature_method == 'spectrogram':\n",
    "            return 201\n",
    "        else:\n",
    "            raise Exception(f'预处理方法 {self.feature_method} 不存在！')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义文件夹路径\n",
    "folder_path = \"/content/drive/MyDrive/GPTData/Haruhi_audio\"\n",
    "\n",
    "audio_path_list = []\n",
    "# 使用os模块列出文件夹中所有的文件和子文件夹\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    for file in files:\n",
    "        # 使用fnmatch模块匹配以.wav结尾的文件\n",
    "        if fnmatch.fnmatch(file, '*.wav'):\n",
    "            # 打印或处理符合条件的文件\n",
    "            audio_path_list.append(os.path.join(root, file))\n",
    "\n",
    "print(type(audio_path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataset = CustomDefDataset(audio_path_list)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AFE = AudioFeatureExtraction()\n",
    "\n",
    "import time\n",
    "since = time.time()\n",
    "\n",
    "feature_np = AFE.feats_extract(dataloader)\n",
    "\n",
    "print(time.time() - since)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
